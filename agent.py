import random
import pickle
import os
from collections import defaultdict

class QLearningRobot:
    """×¨×•×‘×•×˜ Q-Learning ×¢× ××™×¤×•×¡ ×ª×’××•×œ ×—×›× + ××™×¤×•×¡ × ×§×•×“×•×ª ×‘×˜×¢×™× ×” - ×’×¨×¡×” ××©×•×¤×¨×ª"""
    
    def __init__(self, learning_rate=0.1, discount_factor=0.9, epsilon=0.3):
        # ×¤×¨××˜×¨×™ Q-Learning
        self.learning_rate = learning_rate      
        self.discount_factor = discount_factor  
        self.epsilon = epsilon                  
        
        # ××¦×‘ ×”×¨×•×‘×•×˜
        self.x = 19                             
        self.y = 9                             
        self.collision_count = 0               
        self.max_collisions = 5  # ×™×•×ª×¨ ×¡×•×‘×œ× ×•×ª
        self.is_charging = False               
        self.charging_station = (1, 1)        
        
        # ğŸŒŸ ×”×—×“×©: ××¢×§×‘ ××—×¨ × ×™×§×•×“ ××¤×™×–×•×“ × ×•×›×—×™
        self.current_episode_reward = 0
        
        # ×¤×¢×•×œ×•×ª ××¤×©×¨×™×•×ª
        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
        self.action_names = ['×©×××œ', '×™××™×Ÿ', '×œ××¢×œ×”', '×œ××˜×”']
        
        # Q-Table
        self.q_table = defaultdict(lambda: [0.0] * len(self.actions))
        
        # ×¡×˜×˜×™×¡×˜×™×§×•×ª
        self.episode_rewards = []              
        self.episode_steps = []                
        self.episode_collisions = []           
        self.success_episodes = []             
        
        print(f"ğŸ¤– ×¨×•×‘×•×˜ ××©×•×¤×¨ ×¢× ×¢×•× ×© ×§×™×¨×•×ª + ××™×¤×•×¡ ×¨×š × ×•×¦×¨!")
        print(f"ğŸ“Š ×¤×¨××˜×¨×™×: LR={learning_rate}, Î³={discount_factor}, Îµ={epsilon}")
        print(f"âœ¨ ×—×“×©: ×¢×•× ×© ×¢×œ ×”×ª×§×¨×‘×•×ª ×œ×§×™×¨×•×ª!")
        print(f"ğŸ”‹ ×—×“×©: ×—×–×¨×” ×œ×˜×¢×™× ×” = 0 (×œ× -200)!")
    
    def reset(self):
        """××™×¤×•×¡ ×”×¨×•×‘×•×˜ ×œ××¦×‘ ×”×ª×—×œ×ª×™"""
        self.x = 19
        self.y = 9
        self.collision_count = 0
        self.is_charging = False
        # ğŸŒŸ ××™×¤×•×¡ × ×™×§×•×“ ××¤×™×–×•×“
        self.current_episode_reward = 0
    
    def get_position(self):
        """×”×—×–×¨×ª ××™×§×•× ×”×¨×•×‘×•×˜"""
        return (self.x, self.y)
    
    def get_wall_distance(self):
        """×—×™×©×•×‘ ×”××¨×—×§ ×”×§×¨×•×‘ ×‘×™×•×ª×¨ ×œ×§×™×¨"""
        distances = [
            self.x,           # ××¨×—×§ ×œ×§×™×¨ ×©×××œ
            19 - self.x,      # ××¨×—×§ ×œ×§×™×¨ ×™××™×Ÿ
            self.y,           # ××¨×—×§ ×œ×§×™×¨ ×¢×œ×™×•×Ÿ
            9 - self.y        # ××¨×—×§ ×œ×§×™×¨ ×ª×—×ª×•×Ÿ
        ]
        return min(distances)
    
    def encode_state(self, cats_positions, dirt_positions):
        """×§×™×“×•×“ ××¦×‘ ××¤×•×¨×˜ ×™×•×ª×¨ - ××ª××§×“ ×‘×—××§× ×•×ª ××—×ª×•×œ×™× + ××¨×—×§ ××§×™×¨×•×ª"""
        
        # ××™×§×•× ×”×¨×•×‘×•×˜
        robot_zone_x = self.x // 5  # 4 ××–×•×¨×™× ××•×¤×§×™×™×
        robot_zone_y = self.y // 3  # 3 ××–×•×¨×™× ×× ×›×™×™×
        
        # ğŸ†• ××¨×—×§ ××§×™×¨×•×ª
        wall_distance = min(self.get_wall_distance(), 3)  # ××§×¡×™××•× 3 ×œ××˜×¨×ª ×§×™×“×•×“
        
        # × ×™×ª×•×— ×—×ª×•×œ×™× - ×”×™×›×Ÿ ×”× ×™×—×¡×™×ª ×œ×¨×•×‘×•×˜?
        cat_threat_level = 0
        cat_direction = 4  # 0=×©×××œ, 1=×™××™×Ÿ, 2=×œ××¢×œ×”, 3=×œ××˜×”, 4=×¨×—×•×§
        
        if cats_positions:
            min_cat_distance = 999
            closest_cat = None
            
            for cat_pos in cats_positions:
                distance = abs(self.x - cat_pos[0]) + abs(self.y - cat_pos[1])
                if distance < min_cat_distance:
                    min_cat_distance = distance
                    closest_cat = cat_pos
            
            # ×¨××ª ××™×•×
            if min_cat_distance <= 1:
                cat_threat_level = 3  # ×¡×›× ×” ××™×™×“×™×ª!
            elif min_cat_distance <= 2:
                cat_threat_level = 2  # ×¡×›× ×” ×§×¨×•×‘×”
            elif min_cat_distance <= 4:
                cat_threat_level = 1  # ×–×”×™×¨×•×ª
            else:
                cat_threat_level = 0  # ×‘×˜×•×—
            
            # ×›×™×•×•×Ÿ ×”×—×ª×•×œ ×”×™×—×¡×™ ×œ×¨×•×‘×•×˜
            if closest_cat and min_cat_distance <= 5:
                dx = closest_cat[0] - self.x
                dy = closest_cat[1] - self.y
                
                if abs(dx) > abs(dy):  # ×ª× ×•×¢×” ××•×¤×§×™×ª
                    cat_direction = 1 if dx > 0 else 0  # ×™××™×Ÿ ××• ×©×××œ
                else:  # ×ª× ×•×¢×” ×× ×›×™×ª
                    cat_direction = 3 if dy > 0 else 2  # ×œ××˜×” ××• ×œ××¢×œ×”
        
        # × ×™×ª×•×— ×œ×™×›×œ×•×›×™×
        dirt_count = len(dirt_positions)
        closest_dirt_distance = 999
        dirt_direction = 4  # ×›××• ×—×ª×•×œ×™×
        
        if dirt_positions:
            for dirt_pos in dirt_positions:
                distance = abs(self.x - dirt_pos[0]) + abs(self.y - dirt_pos[1])
                if distance < closest_dirt_distance:
                    closest_dirt_distance = distance
                    
                    # ×›×™×•×•×Ÿ ×”×œ×™×›×œ×•×š
                    dx = dirt_pos[0] - self.x
                    dy = dirt_pos[1] - self.y
                    
                    if abs(dx) > abs(dy):
                        dirt_direction = 1 if dx > 0 else 0
                    else:
                        dirt_direction = 3 if dy > 0 else 2
        
        # ××¦×‘ ××¤×•×¨×˜ ×™×•×ª×¨ ×¢× ××¨×—×§ ×§×™×¨×•×ª
        state = (
            robot_zone_x,
            robot_zone_y,
            wall_distance,           # ğŸ†• ××¨×—×§ ××§×™×¨×•×ª
            cat_threat_level,        # ×¨××ª ××™×•× ××—×ª×•×œ×™×
            cat_direction,           # ×›×™×•×•×Ÿ ×”×—×ª×•×œ ×”×§×¨×•×‘
            min(dirt_count, 5),      # ××¡×¤×¨ ×œ×™×›×œ×•×›×™×
            dirt_direction,          # ×›×™×•×•×Ÿ ×”×œ×™×›×œ×•×š ×”×§×¨×•×‘
            min(closest_dirt_distance, 10) // 2,  # ××¨×—×§ ×œ×œ×™×›×œ×•×š
            min(self.collision_count, 5),  # ××¡×¤×¨ ×”×ª× ×’×©×•×™×•×ª
            int(self.is_charging)
        )
        
        return state
    
    def choose_action(self, state):
        """×‘×—×™×¨×ª ×¤×¢×•×œ×” ×¢× ×¢×“×™×¤×•×ª ×œ×—××§× ×•×ª"""
        if random.random() < self.epsilon:
            return random.randint(0, len(self.actions) - 1)
        else:
            q_values = self.q_table[state]
            max_q = max(q_values)
            best_actions = [i for i, q in enumerate(q_values) if abs(q - max_q) < 0.001]
            return random.choice(best_actions)
    
    def is_valid_move(self, new_x, new_y):
        """×‘×“×™×§×” ×× ×”×ª× ×•×¢×” ×ª×§×™× ×”"""
        return 0 <= new_x < 20 and 0 <= new_y < 10
    
    def move(self, action):
        """×‘×™×¦×•×¢ ×ª× ×•×¢×”"""
        if self.is_charging:
            self.is_charging = False
            return True
            
        dx, dy = self.actions[action]
        new_x, new_y = self.x + dx, self.y + dy
        
        if self.is_valid_move(new_x, new_y):
            self.x, self.y = new_x, new_y
            return True
        else:
            return False
    
    def handle_collision(self):
        """×˜×™×¤×•×œ ×‘×”×ª× ×’×©×•×ª ×¢× ×—×ª×•×œ"""
        self.collision_count += 1
        print(f"ğŸ’¥ ×”×ª× ×’×©×•×ª! ×¡×”\"×›: {self.collision_count}/{self.max_collisions}")
        
        if self.collision_count >= self.max_collisions:
            self.x, self.y = self.charging_station
            self.collision_count = 0
            self.is_charging = True
            
            # ğŸ”‹ ×”×—×“×©: ××™×¤×•×¡ × ×§×•×“×•×ª ×›×©×—×•×–×¨ ×œ×˜×¢×™× ×”!
            old_score = self.current_episode_reward
            self.current_episode_reward = 0
            print(f"ğŸ”‹ ×—×–×¨×” ×œ×¢××“×ª ×˜×¢×™× ×”! × ×§×•×“×•×ª ××•×¤×¡×•: {old_score:.0f} â†’ 0 (×œ×œ× ×¢×•× ×©!)")
            return True
        return False
    
    def calculate_reward(self, old_pos, cats_positions, dirt_positions, wall_hit, dirt_collected):
        """××¢×¨×›×ª ×ª×’××•×œ×™× ××©×•×¤×¨×ª ×¢× ×¢×•× ×© ×§×™×¨×•×ª ×•×—×–×¨×” ×¨×›×” ×œ×˜×¢×™× ×”"""
        
        current_pos = (self.x, self.y)
        
        # ğŸŒŸ ×”×¨×¢×™×•×Ÿ ×”×—×›× ×©×œ×š: ××™×¤×•×¡ ×—×›× ×¢×œ ××™×¡×•×£ ×œ×™×›×œ×•×š!
        if dirt_collected:
            if self.current_episode_reward < 0:
                # ×× ×”× ×™×§×•×“ ×©×œ×™×œ×™ - ××™×¤×•×¡ ×œ-0 ×•××– +200
                reset_amount = -self.current_episode_reward  # ×›××” ×¦×¨×™×š ×œ××¤×¡
                total_reward = reset_amount + 200
                print(f"ğŸ§¹ ×œ×™×›×œ×•×š × ××¡×£! × ×™×§×•×“ ×”×™×” {self.current_episode_reward:.0f} â†’ ××™×¤×•×¡ + 200 = +{total_reward:.0f}")
                return total_reward
            else:
                # ×× ×”× ×™×§×•×“ ×—×™×•×‘×™ - ×¤×©×•×˜ +200
                print(f"ğŸ§¹ ×œ×™×›×œ×•×š × ××¡×£! × ×™×§×•×“ ×”×™×” +{self.current_episode_reward:.0f} â†’ +200 = +{self.current_episode_reward + 200:.0f}")
                return 200
        
        # ×× ×œ× ×ª×¤×¡ ×œ×™×›×œ×•×š - ××¢×¨×›×ª ×¢×•× ×©×™× ×¨×’×™×œ×”
        reward = -0.05  # ×¢×•× ×© ×§×˜×Ÿ ×¢×œ ×›×œ ×¦×¢×“
        
        # ×¢×•× ×© ×›×‘×“ ×¢×œ ×”×ª× ×’×©×•×ª
        collision_happened = current_pos in cats_positions
        if collision_happened:
            reward -= 100  # ×¢×•× ×© ×›×‘×“!
            print(f"ğŸ’¥ ×”×ª× ×’×©×•×ª! -100")
            went_to_charging = self.handle_collision()
            if went_to_charging:
                # ğŸ†• ×©×™× ×•×™: ×œ× -200, ×¤×©×•×˜ 0 (×”× ×§×•×“×•×ª ×›×‘×¨ ××•×¤×¡×•)
                print(f"ğŸ”‹ ×—×–×¨×” ×œ×˜×¢×™× ×”! (×œ×œ× ×¢×•× ×© × ×•×¡×£ - × ×§×•×“×•×ª ××•×¤×¡×•)")
        
        # ğŸ†• ×¢×•× ×© ×¢×œ ×”×ª×§×¨×‘×•×ª ×œ×§×™×¨×•×ª
        wall_distance = self.get_wall_distance()
        if wall_distance == 0:
            reward -= 25  # ×¢×•× ×© ×‘×™× ×•× ×™ ×¢×œ ×œ×”×™×•×ª ×¦××•×“ ×œ×§×™×¨
        elif wall_distance == 1:
            reward -= 10  # ×¢×•× ×© ×§×˜×Ÿ ×¢×œ ×œ×”×™×•×ª ×§×¨×•×‘ ×œ×§×™×¨
        elif wall_distance == 2:
            reward -= 3   # ×¢×•× ×© ×§×˜×Ÿ ×××•×“ ×¢×œ ×œ×”×™×•×ª ×“×™ ×§×¨×•×‘
        
        # ×ª×’××•×œ ×¢×œ ×”×ª×¨×—×§×•×ª ××—×ª×•×œ×™×
        if cats_positions:
            current_min_cat_distance = min([abs(self.x - cat[0]) + abs(self.y - cat[1]) 
                                           for cat in cats_positions])
            old_min_cat_distance = min([abs(old_pos[0] - cat[0]) + abs(old_pos[1] - cat[1]) 
                                       for cat in cats_positions])
            
            # ×ª×’××•×œ ×¢×œ ×”×ª×¨×—×§×•×ª ××—×ª×•×œ×™×
            if current_min_cat_distance > old_min_cat_distance:
                reward += 5  # ×ª×’××•×œ ×¢×œ ×—××§× ×•×ª
            elif current_min_cat_distance < old_min_cat_distance:
                reward -= 10  # ×¢×•× ×© ×¢×œ ×”×ª×§×¨×‘×•×ª
            
            # ×¢×•× ×© ×¢×œ ×œ×”×™×•×ª ×§×¨×•×‘ ××“×™
            if current_min_cat_distance <= 1:
                reward -= 30  # ×¡×›× ×” ××™×™×“×™×ª
            elif current_min_cat_distance == 2:
                reward -= 15  # ×¡×›× ×” ×§×¨×•×‘×”
        
        # ×ª×’××•×œ ×¢×œ ×”×ª×§×¨×‘×•×ª ×œ×œ×™×›×œ×•×š (×¨×§ ×× ×œ× ××¡×•×›×Ÿ)
        if dirt_positions and not collision_happened:
            current_min_dirt_distance = min([abs(self.x - dirt[0]) + abs(self.y - dirt[1]) 
                                            for dirt in dirt_positions])
            old_min_dirt_distance = min([abs(old_pos[0] - dirt[0]) + abs(old_pos[1] - dirt[1]) 
                                       for dirt in dirt_positions])
            
            if current_min_dirt_distance < old_min_dirt_distance:
                reward += 3  # ×ª×’××•×œ ×¢×œ ×”×ª×§×¨×‘×•×ª ×œ×œ×™×›×œ×•×š
        
        # ×¢×•× ×© ×¢×œ ×¤×’×™×¢×” ×‘×§×™×¨
        if wall_hit:
            reward -= 50  # ×¢×•× ×© ×›×‘×“ ×›××• ×©×¨×¦×™×ª
        
        # ×‘×•× ×•×¡ ×¢×œ ×¡×™×•× ××©×™××”
        if len(dirt_positions) == 0:
            reward += 500  # ×‘×•× ×•×¡ × ×•×¡×£
            print("ğŸ‰ ××©×™××” ×”×•×©×œ××”! +500")
        
        return reward
    
    def update_q_table(self, state, action, reward, next_state, done):
        """×¢×“×›×•×Ÿ Q-table"""
        current_q = self.q_table[state][action]
        
        if done:
            max_next_q = 0
        else:
            max_next_q = max(self.q_table[next_state])
        
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[state][action] = new_q
    
    def step(self, cats_positions, dirt_positions):
        """×¦×¢×“ ××—×“ ×©×œ ×”×¨×•×‘×•×˜"""
        current_state = self.encode_state(cats_positions, dirt_positions)
        old_pos = (self.x, self.y)
        
        action = self.choose_action(current_state)
        wall_hit = not self.move(action)
        
        current_pos = (self.x, self.y)
        dirt_collected = current_pos in dirt_positions
        
        reward = self.calculate_reward(old_pos, cats_positions, dirt_positions, wall_hit, dirt_collected)
        
        # ğŸŒŸ ×¢×“×›×•×Ÿ ×”× ×™×§×•×“ ×”××¦×˜×‘×¨ ×©×œ ×”××¤×™×–×•×“ (××œ× ×× ×›×Ÿ ××•×¤×¡ ×‘×˜×¢×™× ×”)
        if not (self.x, self.y) == self.charging_station or not self.is_charging:
            self.current_episode_reward += reward
        
        done = len(dirt_positions) == 0
        
        if dirt_collected:
            new_dirt_positions = dirt_positions - {current_pos}
        else:
            new_dirt_positions = dirt_positions
            
        next_state = self.encode_state(cats_positions, new_dirt_positions)
        self.update_q_table(current_state, action, reward, next_state, done)
        
        return reward, done
    
    def decay_epsilon(self, decay_rate=0.995, min_epsilon=0.01):
        """×”×¤×—×ª×ª epsilon"""
        self.epsilon = max(min_epsilon, self.epsilon * decay_rate)
    
    def get_stats(self):
        """×”×—×–×¨×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª"""
        if not self.episode_rewards:
            return {}
        
        def calculate_average(lst, last_n=100):
            if not lst:
                return 0
            recent = lst[-last_n:] if len(lst) > last_n else lst
            return sum(recent) / len(recent)
            
        return {
            'total_episodes': len(self.episode_rewards),
            'average_reward': calculate_average(self.episode_rewards, 100),
            'average_steps': calculate_average(self.episode_steps, 100),
            'success_rate': calculate_average(self.success_episodes, 100) if self.success_episodes else 0,
            'current_epsilon': self.epsilon,
            'q_table_size': len(self.q_table),
            'current_episode_score': self.current_episode_reward,  # ğŸ”‹ × ×§×•×“×•×ª × ×•×›×—×™×•×ª
            'wall_distance': self.get_wall_distance()  # ğŸ†• ××¨×—×§ × ×•×›×—×™ ××§×™×¨×•×ª
        }
    
    def add_episode_stats(self, total_reward, steps, success):
        """×”×•×¡×¤×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª ××¤×™×–×•×“"""
        self.episode_rewards.append(total_reward)
        self.episode_steps.append(steps)
        self.success_episodes.append(1 if success else 0)
    
    def save_model(self, filename='improved_robot.pkl'):
        """×©××™×¨×ª ××•×“×œ"""
        model_data = {
            'q_table': dict(self.q_table),
            'learning_rate': self.learning_rate,
            'discount_factor': self.discount_factor,
            'epsilon': self.epsilon,
            'episode_rewards': self.episode_rewards,
            'episode_steps': self.episode_steps,
            'success_episodes': self.success_episodes
        }
        
        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"ğŸ’¾ ×¨×•×‘×•×˜ ××©×•×¤×¨ × ×©××¨: {filename}")
    
    def load_model(self, filename='improved_robot.pkl'):
        """×˜×¢×™× ×ª ××•×“×œ"""
        if not os.path.exists(filename):
            print(f"âŒ ×§×•×‘×¥ {filename} ×œ× × ××¦×")
            return False
            
        try:
            with open(filename, 'rb') as f:
                model_data = pickle.load(f)
            
            self.q_table = defaultdict(lambda: [0.0] * len(self.actions))
            self.q_table.update(model_data['q_table'])
            self.learning_rate = model_data.get('learning_rate', self.learning_rate)
            self.discount_factor = model_data.get('discount_factor', self.discount_factor)
            self.epsilon = model_data.get('epsilon', self.epsilon)
            self.episode_rewards = model_data.get('episode_rewards', [])
            self.episode_steps = model_data.get('episode_steps', [])
            self.success_episodes = model_data.get('success_episodes', [])
            
            print(f"âœ… ×¨×•×‘×•×˜ ××©×•×¤×¨ × ×˜×¢×Ÿ: {filename}")
            return True
            
        except Exception as e:
            print(f"âŒ ×©×’×™××”: {e}")
            return False


# ×‘×“×™×§×” ××”×™×¨×”
def test_improved_robot():
    """×‘×“×™×§×” ×©×œ ×”×¨×•×‘×•×˜ ×”××©×•×¤×¨"""
    print("ğŸ§ª ×‘×“×™×§×ª ×¨×•×‘×•×˜ ××©×•×¤×¨ ×¢× ×¢×•× ×© ×§×™×¨×•×ª...")
    
    robot = QLearningRobot(learning_rate=0.3, discount_factor=0.95, epsilon=0.5)
    
    # ×‘×“×™×§×ª ××¨×—×§ ××§×™×¨×•×ª
    print(f"ğŸ“ ××™×§×•× ×¨×•×‘×•×˜: {robot.get_position()}")
    print(f"ğŸ§± ××¨×—×§ ××§×™×¨×•×ª: {robot.get_wall_distance()}")
    
    # ×¡×™××•×œ×¦×™×™×ª ××¦×‘
    cats_pos = [(18, 9)]  # ×—×ª×•×œ ×œ×™×“ ×”×¨×•×‘×•×˜
    dirt_pos = {(15, 8)}  # ×œ×™×›×œ×•×š ×¨×—×•×§
    
    print(f"ğŸ± ××™×§×•× ×—×ª×•×œ: {cats_pos}")
    print(f"ğŸ§¹ ××™×§×•× ×œ×™×›×œ×•×š: {dirt_pos}")
    print(f"ğŸ’° × ×§×•×“×•×ª ×”×ª×—×œ×ª×™×•×ª: {robot.current_episode_reward}")
    
    # ×¡×™××•×œ×¦×™×™×ª ×›××” ×¦×¢×“×™×
    for step in range(10):
        reward, done = robot.step(cats_pos, dirt_pos)
        robot_pos = robot.get_position()
        wall_dist = robot.get_wall_distance()
        
        print(f"×¦×¢×“ {step+1}: ××™×§×•× {robot_pos}, ××¨×—×§ ××§×™×¨: {wall_dist}, ×ª×’××•×œ: {reward:.2f}, × ×§×•×“×•×ª: {robot.current_episode_reward:.2f}")
        
        if robot.is_charging:
            print("ğŸ”‹ ×”×¨×•×‘×•×˜ ×‘×˜×¢×™× ×” - ×”× ×§×•×“×•×ª ××•×¤×¡×• (×œ×œ× ×¢×•× ×© × ×•×¡×£)!")
            break
        
        if done:
            print("âœ… ××©×™××” ×”×•×©×œ××”!")
            break


if __name__ == "__main__":
    test_improved_robot()
